{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## H2O gradient boosting and grid search\n",
    "\n",
    "In this tutorial I will show you how to build and tune an H2O gbm model. I will first build a baseline gbm model without tuning the hyperparameters. Then I fine-tune the gbm model with hyperparameters in the gbm function. Then I use h2o.grid to conduct an extensive grid search to optimize the performance. You will see the model performance, measured by the area under the curve (AUC) in our case, has improved dramatically.\n",
    "\n",
    "The dataset is an anonymized credit card [dataset](https://www.kaggle.com/mlg-ulb/creditcardfraud/data) from Kaggle competition. I choose this anonymized dataset so as not to focus on feature engineering but to focus on the detail of h2o.gbm functionalities. \n",
    "\n",
    "The H2O’s GBM supports the following functionalities:\n",
    "* supervised learning for regression and classification tasks\n",
    "* distributed and parallelized computation on either a single node or a multi-node cluster\n",
    "* fast and memory-efficient Java implementations of the underlying algorithms\n",
    "* user-friendly web interface to mirror the model building and scoring process running in R or Python\n",
    "* grid search for hyperparameter optimization and model selection\n",
    "* model export in plain Java code for deployment in production environments\n",
    "* additional parameters for model tuning.\n",
    "\n",
    "\n",
    "We use h2o.init(nthreads=-1) to initilize an h2o environment. \"Number of threads\" is pretty much the number of CPUs used for a laptop. -1 means use all CPUs on the host (Default). A positive integer specifies the number of CPUs directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Connection successful!\n",
      "\n",
      "R is connected to the H2O cluster: \n",
      "    H2O cluster uptime:         3 seconds 988 milliseconds \n",
      "    H2O cluster version:        3.16.0.2 \n",
      "    H2O cluster version age:    4 months and 30 days !!! \n",
      "    H2O cluster name:           H2O_started_from_R_chriskuo_hhp100 \n",
      "    H2O cluster total nodes:    1 \n",
      "    H2O cluster total memory:   1.78 GB \n",
      "    H2O cluster total cores:    4 \n",
      "    H2O cluster allowed cores:  4 \n",
      "    H2O cluster healthy:        TRUE \n",
      "    H2O Connection ip:          localhost \n",
      "    H2O Connection port:        54321 \n",
      "    H2O Connection proxy:       NA \n",
      "    H2O Internal Security:      FALSE \n",
      "    H2O API Extensions:         XGBoost, Algos, AutoML, Core V3, Core V4 \n",
      "    R Version:                  R version 3.4.3 (2017-11-30) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message in h2o.clusterInfo():\n",
      "“\n",
      "Your H2O cluster version is too old (4 months and 30 days)!\n",
      "Please download and install the latest version from http://h2o.ai/download/”"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "library(dplyr)\n",
    "library(h2o)\n",
    "h2o.init(nthreads=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  |======================================================================| 100%\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<ol class=list-inline>\n",
       "\t<li>284807</li>\n",
       "\t<li>31</li>\n",
       "</ol>\n"
      ],
      "text/latex": [
       "\\begin{enumerate*}\n",
       "\\item 284807\n",
       "\\item 31\n",
       "\\end{enumerate*}\n"
      ],
      "text/markdown": [
       "1. 284807\n",
       "2. 31\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "[1] 284807     31"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       " Time             V1                   V2                  \n",
       " Min.   :     0   Min.   :-5.641e+01   Min.   :-7.272e+01  \n",
       " 1st Qu.: 54202   1st Qu.:-9.204e-01   1st Qu.:-5.985e-01  \n",
       " Median : 84692   Median : 1.811e-02   Median : 6.549e-02  \n",
       " Mean   : 94814   Mean   : 1.328e-15   Mean   : 4.088e-16  \n",
       " 3rd Qu.:139320   3rd Qu.: 1.316e+00   3rd Qu.: 8.037e-01  \n",
       " Max.   :172792   Max.   : 2.455e+00   Max.   : 2.206e+01  \n",
       " V3                   V4                   V5                  \n",
       " Min.   :-4.833e+01   Min.   :-5.683e+00   Min.   :-1.137e+02  \n",
       " 1st Qu.:-8.904e-01   1st Qu.:-8.486e-01   1st Qu.:-6.916e-01  \n",
       " Median : 1.798e-01   Median :-1.985e-02   Median :-5.434e-02  \n",
       " Mean   :-1.584e-15   Mean   : 2.210e-15   Mean   : 1.073e-15  \n",
       " 3rd Qu.: 1.027e+00   3rd Qu.: 7.433e-01   3rd Qu.: 6.119e-01  \n",
       " Max.   : 9.383e+00   Max.   : 1.688e+01   Max.   : 3.480e+01  \n",
       " V6                   V7                   V8                  \n",
       " Min.   :-2.616e+01   Min.   :-4.356e+01   Min.   :-7.322e+01  \n",
       " 1st Qu.:-7.683e-01   1st Qu.:-5.541e-01   1st Qu.:-2.086e-01  \n",
       " Median :-2.742e-01   Median : 4.010e-02   Median : 2.236e-02  \n",
       " Mean   : 1.316e-15   Mean   :-4.982e-16   Mean   : 2.299e-16  \n",
       " 3rd Qu.: 3.986e-01   3rd Qu.: 5.704e-01   3rd Qu.: 3.273e-01  \n",
       " Max.   : 7.330e+01   Max.   : 1.206e+02   Max.   : 2.001e+01  \n",
       " V9                   V10                  V11                 \n",
       " Min.   :-1.343e+01   Min.   :-2.459e+01   Min.   :-4.797e+00  \n",
       " 1st Qu.:-6.431e-01   1st Qu.:-5.354e-01   1st Qu.:-7.625e-01  \n",
       " Median :-5.143e-02   Median :-9.292e-02   Median :-3.276e-02  \n",
       " Mean   :-2.203e-15   Mean   : 2.111e-15   Mean   : 2.082e-15  \n",
       " 3rd Qu.: 5.971e-01   3rd Qu.: 4.539e-01   3rd Qu.: 7.396e-01  \n",
       " Max.   : 1.559e+01   Max.   : 2.375e+01   Max.   : 1.202e+01  \n",
       " V12                  V13                  V14                 \n",
       " Min.   :-1.868e+01   Min.   :-5.792e+00   Min.   :-1.921e+01  \n",
       " 1st Qu.:-4.056e-01   1st Qu.:-6.485e-01   1st Qu.:-4.256e-01  \n",
       " Median : 1.400e-01   Median :-1.357e-02   Median : 5.060e-02  \n",
       " Mean   :-1.558e-15   Mean   : 7.057e-16   Mean   : 1.635e-15  \n",
       " 3rd Qu.: 6.182e-01   3rd Qu.: 6.625e-01   3rd Qu.: 4.931e-01  \n",
       " Max.   : 7.848e+00   Max.   : 7.127e+00   Max.   : 1.053e+01  \n",
       " V15                  V16                  V17                 \n",
       " Min.   :-4.499e+00   Min.   :-1.413e+01   Min.   :-2.516e+01  \n",
       " 1st Qu.:-5.829e-01   1st Qu.:-4.680e-01   1st Qu.:-4.837e-01  \n",
       " Median : 4.807e-02   Median : 6.641e-02   Median :-6.568e-02  \n",
       " Mean   : 4.867e-15   Mean   : 1.375e-15   Mean   :-2.299e-16  \n",
       " 3rd Qu.: 6.488e-01   3rd Qu.: 5.233e-01   3rd Qu.: 3.997e-01  \n",
       " Max.   : 8.878e+00   Max.   : 1.732e+01   Max.   : 9.254e+00  \n",
       " V18                  V19                  V20                 \n",
       " Min.   :-9.499e+00   Min.   :-7.214e+00   Min.   :-5.450e+01  \n",
       " 1st Qu.:-4.988e-01   1st Qu.:-4.563e-01   1st Qu.:-2.117e-01  \n",
       " Median :-3.636e-03   Median : 3.735e-03   Median :-6.248e-02  \n",
       " Mean   : 1.130e-15   Mean   : 9.995e-16   Mean   : 6.195e-16  \n",
       " 3rd Qu.: 5.008e-01   3rd Qu.: 4.589e-01   3rd Qu.: 1.330e-01  \n",
       " Max.   : 5.041e+00   Max.   : 5.592e+00   Max.   : 3.942e+01  \n",
       " V21                  V22                  V23                 \n",
       " Min.   :-3.483e+01   Min.   :-1.093e+01   Min.   :-4.481e+01  \n",
       " 1st Qu.:-2.284e-01   1st Qu.:-5.424e-01   1st Qu.:-1.618e-01  \n",
       " Median :-2.945e-02   Median : 6.782e-03   Median :-1.119e-02  \n",
       " Mean   : 1.948e-16   Mean   :-2.555e-16   Mean   : 3.098e-16  \n",
       " 3rd Qu.: 1.864e-01   3rd Qu.: 5.286e-01   3rd Qu.: 1.476e-01  \n",
       " Max.   : 2.720e+01   Max.   : 1.050e+01   Max.   : 2.253e+01  \n",
       " V24                  V25                  V26                 \n",
       " Min.   :-2.837e+00   Min.   :-1.030e+01   Min.   :-2.605e+00  \n",
       " 1st Qu.:-3.546e-01   1st Qu.:-3.171e-01   1st Qu.:-3.270e-01  \n",
       " Median : 4.098e-02   Median : 1.659e-02   Median :-5.214e-02  \n",
       " Mean   : 4.490e-15   Mean   : 6.770e-16   Mean   : 1.705e-15  \n",
       " 3rd Qu.: 4.395e-01   3rd Qu.: 3.507e-01   3rd Qu.: 2.410e-01  \n",
       " Max.   : 4.585e+00   Max.   : 7.520e+00   Max.   : 3.517e+00  \n",
       " V27                  V28                  Amount            \n",
       " Min.   :-2.257e+01   Min.   :-1.543e+01   Min.   :    0.00  \n",
       " 1st Qu.:-7.084e-02   1st Qu.:-5.296e-02   1st Qu.:    5.60  \n",
       " Median : 1.342e-03   Median : 1.124e-02   Median :   22.00  \n",
       " Mean   :-3.641e-16   Mean   :-1.231e-16   Mean   :   88.35  \n",
       " 3rd Qu.: 9.105e-02   3rd Qu.: 7.828e-02   3rd Qu.:   77.17  \n",
       " Max.   : 3.161e+01   Max.   : 3.385e+01   Max.   :25691.16  \n",
       " Class             \n",
       " Min.   :0.000000  \n",
       " 1st Qu.:0.000000  \n",
       " Median :0.000000  \n",
       " Mean   :0.001727  \n",
       " 3rd Qu.:0.000000  \n",
       " Max.   :1.000000  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df <- h2o.importFile(path = \"/Users/chriskuo/Downloads/creditcard.csv\")\n",
    "dim(df)\n",
    "summary(df,exact_quantiles=TRUE)\n",
    "\n",
    "# Specify the response variable\n",
    "response <- \"Class\"\n",
    "\n",
    "# Make the response variable a categorical variable\n",
    "df[[response]] <- as.factor(df[[response]])           \n",
    "\n",
    "## Exclude the variable 'Time' \n",
    "predictors <- setdiff(names(df), c(response, \"Time\")) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split the data\n",
    "\n",
    "Below is the standard syntax of h2o to split the dataset for training and testing purpose. In order to run and test on small samples, I use 10% for training and 10% for validation. H2o requires only two ratios. The third one is implied. So the test dataset is 90% (but I will not use it.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "splits <- h2o.splitFrame(\n",
    "  data = df, \n",
    "  ratios = c(0.1,0.1),   # the ratios should sum up to to be less than 1.0. \n",
    "    destination_frames = c(\"train\", \"valid\", \"test\"), seed = 1234\n",
    ")\n",
    "train <- splits[[1]]\n",
    "valid <- splits[[2]]\n",
    "test  <- splits[[3]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build a baseline gbm model without hyper-parameter tuning\n",
    "\n",
    "Below I just use all the default values for the hyperpamaters. The AUC on the validation data is 0.569. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  |======================================================================| 100%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Model Details:\n",
       "==============\n",
       "\n",
       "H2OBinomialModel: gbm\n",
       "Model ID:  GBM_model_R_1525121768299_63 \n",
       "Model Summary: \n",
       "  number_of_trees number_of_internal_trees model_size_in_bytes min_depth\n",
       "1              50                       50                8421         5\n",
       "  max_depth mean_depth min_leaves max_leaves mean_leaves\n",
       "1         5    5.00000          7         14     8.44000\n",
       "\n",
       "\n",
       "H2OBinomialMetrics: gbm\n",
       "** Reported on training data. **\n",
       "\n",
       "MSE:  0.000581082\n",
       "RMSE:  0.02410564\n",
       "LogLoss:  0.01538777\n",
       "Mean Per-Class Error:  0.1342519\n",
       "AUC:  0.7444208\n",
       "Gini:  0.4888416\n",
       "\n",
       "Confusion Matrix (vertical: actual; across: predicted) for F1-optimal threshold:\n",
       "           0  1    Error       Rate\n",
       "0      28414  6 0.000211   =6/28420\n",
       "1         11 30 0.268293     =11/41\n",
       "Totals 28425 36 0.000597  =17/28461\n",
       "\n",
       "Maximum Metrics: Maximum metrics at their respective thresholds\n",
       "                        metric threshold    value idx\n",
       "1                       max f1  0.281354 0.779221   8\n",
       "2                       max f2  0.281354 0.750000   8\n",
       "3                 max f0point5  0.986199 0.859873   3\n",
       "4                 max accuracy  0.986199 0.999438   3\n",
       "5                max precision  0.986199 0.931034   3\n",
       "6                   max recall  0.000000 1.000000 297\n",
       "7              max specificity  1.000000 0.999930   0\n",
       "8             max absolute_mcc  0.986199 0.782771   3\n",
       "9   max min_per_class_accuracy  0.281354 0.731707   8\n",
       "10 max mean_per_class_accuracy  0.281354 0.865748   8\n",
       "\n",
       "Gains/Lift Table: Extract with `h2o.gainsLift(<model>, <data>)` or `h2o.gainsLift(<model>, valid=<T/F>, xval=<T/F>)`\n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "0.569700680579612"
      ],
      "text/latex": [
       "0.569700680579612"
      ],
      "text/markdown": [
       "0.569700680579612"
      ],
      "text/plain": [
       "[1] 0.5697007"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "gbm <- h2o.gbm(x = predictors, y = response, training_frame = train)\n",
    "gbm\n",
    "\n",
    "## Get the AUC on the validation set\n",
    "h2o.auc(h2o.performance(gbm, newdata = valid)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find-tune hyper-parameters in h2o.gbm\n",
    "\n",
    "The overall strategy is to test more trees and smaller learning rate. The hyperparameters for tuning are the following:\n",
    "\n",
    "* Learning rate (shrinkage)\n",
    "* Number of trees\n",
    "* Interaction depth\n",
    "* Minimum observation in a node\n",
    "* Bag fraction (fraction of randomly selected observations)\n",
    "\n",
    "\n",
    "The learning rate, a value between 0 and 1, corresponds to how quickly the error is corrected from each tree to the next. A small learning rate will result in long computational time, and a large learning rate makes the system unable to settle down. It will be efficient if the learning rate can decay over time. Therefore there is a hyperparameter to decay the learning rate called the \"learn_rate_annealing\". \"Annealing\", in materials science, describes a heating process that heats up in the beginning then cools down slowly. In gbm a common way to decay the learning rate is call the \"step decay\". It reduces the learning rate by some factor in every few iterations or epochs. Typical values are to reduce the learning rate by a half every 5 epochs. Because we use learning_rate_annealing, we can start with a large learning rate=0.05.\n",
    "\n",
    "* learn_rate= 0.05.\n",
    "* learn_rate_annealing=0.99.\n",
    "* ntrees = 1000.\n",
    "* max_runtime_secs=1200. Early stopping based on timeout. In this case no more than 1200 seconds.\n",
    "* stopping_rounds = 5.\n",
    "* stopping_tolerance = 1e-4.\n",
    "* stopping_metric = \"AUC\". The above three hyperparameters control the early stopping when the AUC does not improve by at least 0.01% for 5 consecutive scoring events.\n",
    "* score_tree_interval = 10. Score every 10 trees to make early stopping reproducible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  |======================================================================| 100%\n",
      "[1] 0.9517869\n"
     ]
    }
   ],
   "source": [
    "gbm <- h2o.gbm(x = predictors, y = response, \n",
    "               training_frame = train, \n",
    "               validation_frame = valid,\n",
    "               learn_rate = .05, learn_rate_annealing =.99,\n",
    "               ntrees=1000,\n",
    "               stopping_rounds = 5,\n",
    "               stopping_tolerance = 1e-4,\n",
    "               stopping_metric = \"AUC\", \n",
    "               seed = 1234)\n",
    "\n",
    "# print the auc for the validation data\n",
    "print(h2o.auc(gbm, valid = TRUE))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fine-turn the hyper-parameters using h2o.grid\n",
    "\n",
    "You can type \"?h2o.grid()\" to understand the grid serach options. The h2o.grid() reserves the following commands:\n",
    "\n",
    "* hyper_params: List of lists of hyper parameters. \n",
    "* search_criteria: The default strategy 'Cartesian' covers the entire space of hyperparameter combinations. For example, if you have three hyperparameters and you have 2, 4, 6 values for each, the Catesian search will result in $2 * 4 * 6 = 48$ models. The alternative is 'RandomDiscrete' strategy to get random search of all the combinations of your hyperparameters. \n",
    "* algorithm: Which algorithm.\n",
    "* grid_id: An id that we can retrieve it later. In this example is \"my_grid\".\n",
    "* ntrees: The number of trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  |======================================================================| 100%\n",
      "Hyper-parameter: max_depth, 14\n",
      "Hyper-parameter: ntrees, 1000\n",
      "[2018-04-30 19:04:50] failure_details: NA \n",
      "[2018-04-30 19:04:50] failure_stack_traces: water.Job$JobCancelledException\n",
      "\tat hex.tree.SharedTree$Driver.scoreAndBuildTrees(SharedTree.java:437)\n",
      "\tat hex.tree.SharedTree$Driver.computeImpl(SharedTree.java:352)\n",
      "\tat hex.ModelBuilder$Driver.compute2(ModelBuilder.java:206)\n",
      "\tat hex.ModelBuilder.trainModelNested(ModelBuilder.java:262)\n",
      "\tat hex.grid.GridSearch.startBuildModel(GridSearch.java:332)\n",
      "\tat hex.grid.GridSearch.buildModel(GridSearch.java:314)\n",
      "\tat hex.grid.GridSearch.gridSearch(GridSearch.java:213)\n",
      "\tat hex.grid.GridSearch.access$000(GridSearch.java:68)\n",
      "\tat hex.grid.GridSearch$1.compute2(GridSearch.java:135)\n",
      "\tat water.H2O$H2OCountedCompleter.compute(H2O.java:1263)\n",
      "\tat jsr166y.CountedCompleter.exec(CountedCompleter.java:468)\n",
      "\tat jsr166y.ForkJoinTask.doExec(ForkJoinTask.java:263)\n",
      "\tat jsr166y.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:974)\n",
      "\tat jsr166y.ForkJoinPool.runWorker(ForkJoinPool.java:1477)\n",
      "\tat jsr166y.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:104)\n",
      " \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "H2O Grid Details\n",
       "================\n",
       "\n",
       "Grid ID: my_grid \n",
       "Used hyper parameters: \n",
       "  -  max_depth \n",
       "  -  ntrees \n",
       "Number of models: 172 \n",
       "Number of failed models: 1 \n",
       "\n",
       "Hyper-Parameter Search Summary: ordered by increasing logloss\n",
       "  max_depth ntrees         model_ids              logloss\n",
       "1         2   1700 my_grid_model_144 0.020718249226475475\n",
       "2         2    300 my_grid_model_116 0.020718249226475475\n",
       "3         2   1000   my_grid_model_0 0.020718249226475475\n",
       "4         2   3000  my_grid_model_30 0.020718249226475475\n",
       "5         2   1800  my_grid_model_12 0.020718249226475475\n",
       "\n",
       "---\n",
       "    max_depth ntrees        model_ids              logloss\n",
       "167         2   2400 my_grid_model_64 0.051342688610633055\n",
       "168         2   3000 my_grid_model_76 0.051342688610633055\n",
       "169         2   1400 my_grid_model_44 0.051342688610633055\n",
       "170         2   1000 my_grid_model_36 0.051342688610633055\n",
       "171         2   2200 my_grid_model_60 0.051342688610633055\n",
       "172         2   1800 my_grid_model_52 0.051342688610633055\n",
       "Failed models\n",
       "-------------\n",
       " max_depth ntrees status_failed msgs_failed\n",
       "        14   1000          FAIL        \"NA\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "hyper_params = list( ntrees = seq(100,3000,200), \n",
    "                    max_depth=seq(2,12,3)   )\n",
    "\n",
    "grid <- h2o.grid(\n",
    "  hyper_params = hyper_params,\n",
    "  \n",
    "  search_criteria = list(strategy = \"Cartesian\"),\n",
    "  \n",
    "  algorithm=\"gbm\",\n",
    "  \n",
    "  grid_id=\"my_grid\",\n",
    "  \n",
    "  # Below are is the same as h2o.gbm()\n",
    "  x = predictors, \n",
    "  y = response, \n",
    "  training_frame = train, \n",
    "  validation_frame = valid,\n",
    "  learn_rate = 0.05,                                                         \n",
    "  learn_rate_annealing = 0.99,                                               \n",
    "  sample_rate = 0.8,                                                       \n",
    "  col_sample_rate = 0.8, \n",
    "  seed = 1234,                                                             \n",
    "  stopping_rounds = 5,\n",
    "  stopping_tolerance = 1e-4,\n",
    "  stopping_metric = \"AUC\", \n",
    "  score_tree_interval = 10                                                \n",
    ")\n",
    "\n",
    "grid        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyper-parameter: max_depth, 14\n",
      "Hyper-parameter: ntrees, 1000\n",
      "[2018-04-30 19:05:41] failure_details: NA \n",
      "[2018-04-30 19:05:41] failure_stack_traces: water.Job$JobCancelledException\n",
      "\tat hex.tree.SharedTree$Driver.scoreAndBuildTrees(SharedTree.java:437)\n",
      "\tat hex.tree.SharedTree$Driver.computeImpl(SharedTree.java:352)\n",
      "\tat hex.ModelBuilder$Driver.compute2(ModelBuilder.java:206)\n",
      "\tat hex.ModelBuilder.trainModelNested(ModelBuilder.java:262)\n",
      "\tat hex.grid.GridSearch.startBuildModel(GridSearch.java:332)\n",
      "\tat hex.grid.GridSearch.buildModel(GridSearch.java:314)\n",
      "\tat hex.grid.GridSearch.gridSearch(GridSearch.java:213)\n",
      "\tat hex.grid.GridSearch.access$000(GridSearch.java:68)\n",
      "\tat hex.grid.GridSearch$1.compute2(GridSearch.java:135)\n",
      "\tat water.H2O$H2OCountedCompleter.compute(H2O.java:1263)\n",
      "\tat jsr166y.CountedCompleter.exec(CountedCompleter.java:468)\n",
      "\tat jsr166y.ForkJoinTask.doExec(ForkJoinTask.java:263)\n",
      "\tat jsr166y.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:974)\n",
      "\tat jsr166y.ForkJoinPool.runWorker(ForkJoinPool.java:1477)\n",
      "\tat jsr166y.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:104)\n",
      " \n",
      "H2O Grid Details\n",
      "================\n",
      "\n",
      "Grid ID: my_grid \n",
      "Used hyper parameters: \n",
      "  -  max_depth \n",
      "  -  ntrees \n",
      "Number of models: 172 \n",
      "Number of failed models: 1 \n",
      "\n",
      "Hyper-Parameter Search Summary: ordered by decreasing auc\n",
      "  max_depth ntrees        model_ids                auc\n",
      "1         5   1000 my_grid_model_37 0.9660330023238469\n",
      "2         5   2600 my_grid_model_69 0.9660330023238469\n",
      "3         5   2000 my_grid_model_57 0.9660330023238469\n",
      "4         5   1800 my_grid_model_53 0.9660330023238469\n",
      "5         5   2200 my_grid_model_61 0.9660330023238469\n",
      "\n",
      "---\n",
      "    max_depth ntrees        model_ids                auc\n",
      "167         2   2400 my_grid_model_64 0.2941364816561296\n",
      "168         2   3000 my_grid_model_76 0.2941364816561296\n",
      "169         2   1400 my_grid_model_44 0.2941364816561296\n",
      "170         2   1000 my_grid_model_36 0.2941364816561296\n",
      "171         2   2200 my_grid_model_60 0.2941364816561296\n",
      "172         2   1800 my_grid_model_52 0.2941364816561296\n",
      "Failed models\n",
      "-------------\n",
      " max_depth ntrees status_failed msgs_failed\n",
      "        14   1000          FAIL        \"NA\"\n"
     ]
    }
   ],
   "source": [
    "## sort the grid models by decreasing AUC\n",
    "sortedGrid <- h2o.getGrid(\"my_grid\", sort_by=\"auc\", decreasing = TRUE)    \n",
    "print(sortedGrid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can print out the top 10 models from the grid search. Below the AUC has increased to 0.96."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] 0.966033\n",
      "[1] 0.966033\n",
      "[1] 0.966033\n",
      "[1] 0.966033\n",
      "[1] 0.966033\n",
      "[1] 0.966033\n",
      "[1] 0.966033\n",
      "[1] 0.966033\n",
      "[1] 0.966033\n",
      "[1] 0.966033\n"
     ]
    }
   ],
   "source": [
    "for (i in 1:10) {\n",
    "  gbm <- h2o.getModel(sortedGrid@model_ids[[i]])\n",
    "  print(h2o.auc(h2o.performance(gbm, valid = TRUE)))\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also understand the details of the best model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Details:\n",
      "==============\n",
      "\n",
      "H2OBinomialModel: gbm\n",
      "Model Key:  my_grid_model_37 \n",
      "Model Summary: \n",
      "  number_of_trees number_of_internal_trees model_size_in_bytes min_depth\n",
      "1             160                      160               34446         5\n",
      "  max_depth mean_depth min_leaves max_leaves mean_leaves\n",
      "1         5    5.00000          6         16    12.16250\n",
      "\n",
      "H2OBinomialMetrics: gbm\n",
      "** Reported on training data. **\n",
      "\n",
      "MSE:  0.0004033557\n",
      "RMSE:  0.02008372\n",
      "LogLoss:  0.01243093\n",
      "Mean Per-Class Error:  0.0001759324\n",
      "AUC:  0.9997854\n",
      "Gini:  0.9995709\n",
      "\n",
      "Confusion Matrix (vertical: actual; across: predicted) for F1-optimal threshold:\n",
      "           0  1    Error       Rate\n",
      "0      28410 10 0.000352  =10/28420\n",
      "1          0 41 0.000000      =0/41\n",
      "Totals 28410 51 0.000351  =10/28461\n",
      "\n",
      "Maximum Metrics: Maximum metrics at their respective thresholds\n",
      "                        metric threshold    value idx\n",
      "1                       max f1  0.123504 0.891304   8\n",
      "2                       max f2  0.123504 0.953488   8\n",
      "3                 max f0point5  0.123504 0.836735   8\n",
      "4                 max accuracy  0.123504 0.999649   8\n",
      "5                max precision  0.123504 0.803922   8\n",
      "6                   max recall  0.123504 1.000000   8\n",
      "7              max specificity  1.000000 0.999648   0\n",
      "8             max absolute_mcc  0.123504 0.896459   8\n",
      "9   max min_per_class_accuracy  0.123504 0.999648   8\n",
      "10 max mean_per_class_accuracy  0.123504 0.999824   8\n",
      "\n",
      "Gains/Lift Table: Extract with `h2o.gainsLift(<model>, <data>)` or `h2o.gainsLift(<model>, valid=<T/F>, xval=<T/F>)`\n",
      "H2OBinomialMetrics: gbm\n",
      "** Reported on validation data. **\n",
      "\n",
      "MSE:  0.001113544\n",
      "RMSE:  0.0333698\n",
      "LogLoss:  0.02273147\n",
      "Mean Per-Class Error:  0.1473383\n",
      "AUC:  0.966033\n",
      "Gini:  0.932066\n",
      "\n",
      "Confusion Matrix (vertical: actual; across: predicted) for F1-optimal threshold:\n",
      "           0  1    Error       Rate\n",
      "0      28613 16 0.000559  =16/28629\n",
      "1         15 36 0.294118     =15/51\n",
      "Totals 28628 52 0.001081  =31/28680\n",
      "\n",
      "Maximum Metrics: Maximum metrics at their respective thresholds\n",
      "                        metric threshold    value idx\n",
      "1                       max f1  0.999537 0.699029   2\n",
      "2                       max f2  0.999537 0.703125   2\n",
      "3                 max f0point5  1.000000 0.697211   0\n",
      "4                 max accuracy  1.000000 0.998919   0\n",
      "5                max precision  1.000000 0.700000   0\n",
      "6                   max recall  0.000000 1.000000 399\n",
      "7              max specificity  1.000000 0.999476   0\n",
      "8             max absolute_mcc  0.999537 0.698521   2\n",
      "9   max min_per_class_accuracy  0.000161 0.941176 318\n",
      "10 max mean_per_class_accuracy  0.000161 0.941876 318\n",
      "\n",
      "Gains/Lift Table: Extract with `h2o.gainsLift(<model>, <data>)` or `h2o.gainsLift(<model>, valid=<T/F>, xval=<T/F>)`\n",
      "\n",
      "\n",
      "Scoring History: \n",
      "             timestamp   duration number_of_trees training_rmse\n",
      "1  2018-04-30 17:13:21  2.397 sec               0       0.03793\n",
      "2  2018-04-30 17:13:21  2.728 sec              10       0.02435\n",
      "3  2018-04-30 17:13:22  3.182 sec              20       0.02325\n",
      "4  2018-04-30 17:13:22  3.553 sec              30       0.02300\n",
      "5  2018-04-30 17:13:22  3.929 sec              40       0.02208\n",
      "6  2018-04-30 17:13:23  4.324 sec              50       0.02197\n",
      "7  2018-04-30 17:13:23  4.704 sec              60       0.02165\n",
      "8  2018-04-30 17:13:23  5.086 sec              70       0.02138\n",
      "9  2018-04-30 17:13:24  5.470 sec              80       0.02109\n",
      "10 2018-04-30 17:13:24  5.849 sec              90       0.02092\n",
      "11 2018-04-30 17:13:25  6.236 sec             100       0.02083\n",
      "12 2018-04-30 17:13:25  6.614 sec             110       0.02072\n",
      "13 2018-04-30 17:13:25  6.992 sec             120       0.02058\n",
      "14 2018-04-30 17:13:26  7.377 sec             130       0.02044\n",
      "15 2018-04-30 17:13:26  7.754 sec             140       0.02032\n",
      "16 2018-04-30 17:13:27  8.251 sec             150       0.02019\n",
      "17 2018-04-30 17:13:27  8.662 sec             160       0.02008\n",
      "   training_logloss training_auc training_lift training_classification_error\n",
      "1           0.01086      0.50000       1.00000                       0.99856\n",
      "2           0.01429      0.98641      88.99625                       0.00056\n",
      "3           0.01357      0.99672      96.07899                       0.00053\n",
      "4           0.01338      0.99673      96.41260                       0.00049\n",
      "5           0.01309      0.99699      97.42747                       0.00042\n",
      "6           0.01284      0.99977      99.86316                       0.00046\n",
      "7           0.01276      0.99977      99.86316                       0.00046\n",
      "8           0.01269      0.99978      99.86316                       0.00039\n",
      "9           0.01263      0.99978      99.16725                       0.00039\n",
      "10          0.01259      0.99978      98.48097                       0.00039\n",
      "11          0.01257      0.99978      99.86316                       0.00039\n",
      "12          0.01254      0.99978      99.86316                       0.00039\n",
      "13          0.01252      0.99978      99.86316                       0.00039\n",
      "14          0.01249      0.99978      99.51399                       0.00039\n",
      "15          0.01247      0.99978      99.86316                       0.00039\n",
      "16          0.01245      0.99979      99.86316                       0.00035\n",
      "17          0.01243      0.99979      99.86316                       0.00035\n",
      "   validation_rmse validation_logloss validation_auc validation_lift\n",
      "1          0.04213            0.01307        0.50000         1.00000\n",
      "2          0.03293            0.02322        0.95329        71.12111\n",
      "3          0.03291            0.02283        0.95406        80.07012\n",
      "4          0.03296            0.02267        0.95489        77.50377\n",
      "5          0.03298            0.02256        0.95473        84.25497\n",
      "6          0.03372            0.02273        0.94709        84.25497\n",
      "7          0.03358            0.02268        0.94739        83.96242\n",
      "8          0.03347            0.02268        0.96736        83.96242\n",
      "9          0.03343            0.02269        0.96729        84.25497\n",
      "10         0.03340            0.02268        0.96727        82.81225\n",
      "11         0.03339            0.02270        0.96725        83.96242\n",
      "12         0.03337            0.02272        0.96727        83.67189\n",
      "13         0.03339            0.02273        0.96727        84.25497\n",
      "14         0.03338            0.02274        0.96692        86.21439\n",
      "15         0.03337            0.02274        0.96646        86.21439\n",
      "16         0.03337            0.02273        0.96612        86.21439\n",
      "17         0.03337            0.02273        0.96603        86.21439\n",
      "   validation_classification_error\n",
      "1                          0.99822\n",
      "2                          0.00105\n",
      "3                          0.00108\n",
      "4                          0.00108\n",
      "5                          0.00108\n",
      "6                          0.00108\n",
      "7                          0.00108\n",
      "8                          0.00108\n",
      "9                          0.00108\n",
      "10                         0.00108\n",
      "11                         0.00108\n",
      "12                         0.00108\n",
      "13                         0.00108\n",
      "14                         0.00108\n",
      "15                         0.00108\n",
      "16                         0.00108\n",
      "17                         0.00108\n",
      "\n",
      "Variable Importances: (Extract with `h2o.varimp`) \n",
      "=================================================\n",
      "\n",
      "Variable Importances: \n",
      "  variable relative_importance scaled_importance percentage\n",
      "1      V14          539.969971          1.000000   0.555501\n",
      "2      V11          370.390594          0.685947   0.381044\n",
      "3      V17           38.789841          0.071837   0.039906\n",
      "4      V25            6.749026          0.012499   0.006943\n",
      "5       V4            3.464664          0.006416   0.003564\n",
      "\n",
      "---\n",
      "   variable relative_importance scaled_importance percentage\n",
      "24       V7            0.014600          0.000027   0.000015\n",
      "25      V26            0.009862          0.000018   0.000010\n",
      "26       V8            0.007270          0.000013   0.000007\n",
      "27      V10            0.001633          0.000003   0.000002\n",
      "28      V18            0.000000          0.000000   0.000000\n",
      "29      V28            0.000000          0.000000   0.000000\n"
     ]
    }
   ],
   "source": [
    "best_model <- h2o.getModel(sortedGrid@model_ids[[1]])\n",
    "summary(best_model)\n",
    "\n",
    "scoring_history <- as.data.frame(best_model@model$scoring_history)\n",
    "#plot(scoring_history$number_of_trees, scoring_history$training_MSE, type=\"p\") #training mse\n",
    "#points(scoring_history$number_of_trees, scoring_history$validation_MSE, type=\"l\") #validation mse\n",
    "\n",
    "## get the actual number of trees\n",
    "ntrees <- best_model@model$model_summary$number_of_trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
